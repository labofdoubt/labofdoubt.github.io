---
title: "Normalization-free transformers are subcritical"
date: 2025-12-20
description: "What do Transformers with pointwise normalization functions trade off to achieve better computational efficiency?"
---

## Abstract

In this blog post, we connect recent work proposing LayerNorm-free Transformers [@zhu2025transformersnormalization; @chen2025strongernormalizationfreetransformers], in which (pre-)LayerNorm (LN) is replaced by pointwise activation functions such as $\tanh$ or $\text{erf}$, with the body of work studying criticality in wide neural networks at initialization. We note that, in neural networks with residual connections, $\tanh$-like normalization functions are subcritical, whereas LayerNorm achieves criticality, as previously shown in [@doshi2023criticalinitializationwidedeep; @yang2017meanfieldresidualnetworks]. In practical terms, residual networks with Dynamic $\tanh$ (DyT) or Dynamic $\text{erf}$ (Derf) normalization exhibit worse gradient propagation than pre-LN residual networks: gradients grow stretched-exponentially with depth (from the last layer to the first) rather than following a power law. This can cause training instability and may require more careful hyperparameter tuning to avoid divergence.

We analyze how the initialization of the parameter $\alpha$, which appears in DyT/Derf, affects gradient propagation in the model proposed in [@doshi2023criticalinitializationwidedeep]. For networks of finite depth, smaller values of $\alpha$ give rise to exponentially growing gradients, whereas larger values of $\alpha$ give rise to stretched-exponentially growing gradients. We show that the qualitative gradient behavior in ViT aligns well with the theory, even though the theory does not account for attention blocks. Larger values of $\alpha$ lead to stronger gradient amplification, which may explain several empirical observations in DyT/Derf models: training is typically more stable at smaller <span class="nowrap">$\alpha$,</span> while overly large $\alpha$ can lead to divergence; deeper models require smaller <span class="nowrap">$\alpha$.</span> In addition, we empirically show that DyT/Derf models benefit from learning rate warm-up, similarly to post-LayerNorm models that suffer from vanishing gradients.

## How to read it?

The blog contains the following sections:

- **1.** [Background](#background)
  - **1.1** [Criticality: large picture](#criticality-large-picture)
  - **1.2** [Criticality: large-width limit and mean-field formalism](#criticality-mean-field)
- **2.** [Analysis of criticality in layernorm-free transformers](#analysis-layernorm-free)
  - **2.1** [LayerNorm vs. tanh-like nonlinearity: criticality perspective](#layernorm-vs-tanh)
  - **2.2** [Introducing $\alpha$: finite-depth transition from exponential to stretched-exponential behavior](#introducing-alpha)
  - **2.3** [Empirical gradient norms in a Transformer with DyT](#empirical-dyt)
  - **2.4** [DyT benefits from learning rate warm-up](#dyt-warmup)
- **3.** [Conclusion](#conclusion)
- **4.** [References](#references)

Readers interested only in the empirical results may want to proceed directly to Sections [**2.3**](#empirical-dyt) and [**2.4**](#dyt-warmup). Readers who want a theoretical explanation but are already familiar with criticality and mean-field theory at initialization may want to start from Section [**2.1**](#layernorm-vs-tanh), although Section [**1.2**](#criticality-mean-field) may be useful for the notation. Readers who want the full picture from scratch may want to start from Section [**1.1**](#criticality-large-picture).

Sections [**1.1**](#criticality-large-picture)–[**2.1**](#layernorm-vs-tanh) review existing theoretical results and provide a gentle introduction to the topic. Namely, Section [**1.1**](#criticality-large-picture) introduces the ordered and chaotic phases, as well as criticality, for signal propagation in neural networks. Section [**1.2**](#criticality-mean-field) introduces the mean-field formalism in the large-width limit, which allows one to quantify signal-propagation properties. Section [**2.1**](#layernorm-vs-tanh) compares the signal-propagation behavior of a toy residual network with LayerNorm and with a $\tanh$-like nonlinearity, and shows theoretically why the former leads to better gradient propagation.

Section [**2.2**](#introducing-alpha) extends the theoretical analysis by introducing the parameter $\alpha$, which rescales the input to the nonlinearity. Section [**2.3**](#empirical-dyt) studies signal propagation empirically in a Transformer model and compares DyT models to the pre-LN baseline. Section [**2.4**](#dyt-warmup) studies the effect of warm-up on training stability for DyT models and compares it to the pre-LN baseline.

<span id="background"></span>
## 1. Background

<span id="criticality-large-picture"></span>
### 1.1 Criticality: large picture

It has been known for some time that a network’s signal-propagation properties at initialization directly affect its training stability, which in turn is strongly correlated with the final performance [@poole2016exponentialexpressivitydeepneural; @schoenholz2017deepinformationpropagation; @doshi2023criticalinitializationwidedeep; @yang2017meanfieldresidualnetworks]. As a striking example, modern Transformers commonly use residual connections and pre-LayerNorm (rather than post-LayerNorm or no normalization) – both of which are known to improve gradient propagation and help prevent exponentially vanishing or exploding gradients [@he2015deepresiduallearningimage; @ba2016layernormalization; @xiong2020layernormalizationtransformerarchitecture; @kedia2024transformersstableendtoendsignal].

The notion of “good” or “bad” signal propagation can be formalized by introducing the **partial Jacobian** $J^{l, l_0}$ between the model’s representations at layers $l_0$ and $l$, with $l_0<l$. This Jacobian determines how perturbations to activations at layer $l_0$ propagate to layer $l$ in the forward pass and how gradients propagate from layer $l$ back to layer $l_0$ in the backward pass. Its squared Frobenius norm, averaged over weight initializations – $\mathcal{J}^{l, l_0}$ (the **averaged partial Jacobian norm**, **APJN**) – is the simplest scalar measure that relates the typical gradient magnitudes at layers $l$ and $l_0$ (details below). 

For a given neural network architecture, the partial Jacobian depends on hyperparameters such as the initialization variances of weights and biases, as well as less standard ones such as residual scaling and the parameter $\alpha$ in DyT/Derf. Thus, each point in hyperparameter space may be characterized by the resulting APJN behavior.

If the APJN grows (or decays) exponentially with depth, $\mathcal{J}^{l, l_0}\sim e^{\pm l/\xi}$, the model is said to be in the **chaotic** (or **ordered**) phase. The depth scale $\xi$, called the **correlation length**, determines the effective depth over which signals propagate efficiently. Empirically, it has been shown that when the model depth substantially exceeds $\xi$, training becomes unstable [@poole2016exponentialexpressivitydeepneural; @schoenholz2017deepinformationpropagation; @doshi2023criticalinitializationwidedeep]. Therefore, to achieve stable training, it is desirable to make $\xi$ as large as possible. 

Typically, as one approaches the region of hyperparameters where $\xi$ diverges, the asymptotic APJN scaling switches to a power law, $\mathcal{J}^{l, l_0}\sim l^{\zeta}$, with critical exponent $\zeta$. In this case, the model is said to be at **criticality**.

Let us illustrate these ideas using the figures from [@doshi2023criticalinitializationwidedeep]. That work studies a residual network of the form
<span id="eq-residual-update" class="eq-anchor"></span>
$$
\begin{equation}
h^{l+1} = \mu h^l + W\phi(\tilde h^{l}) + b,
\end{equation}
$$
where $h^l \in \mathbb{R}^{N_l}$, $W\in \mathbb{R}^{N_{l+1}\times N_l}$, and $b \in \mathbb{R}^{N_{l+1}}$. Here $\tilde {h}^{l}$ is either $\text{LayerNorm}(h^{l})$ or simply $h^{l}$ if no normalization layer is used. We take $h^0$ to be the initial embedding. The components of the weight matrix $W$ are initialized from $\mathcal{N}(0, \sigma_w^2/N_l)$, and the components of the bias vector $b$ are initialized from $\mathcal{N}(0, \sigma_b^2)$. The parameter $\mu$, which is typically set to $1$ in modern transformers, is the residual scaling that controls the contribution of the residual stream. When $\mu = 0$, the model reduces to a feedforward network without residual connections. 

<figure id="fig-phase_diag" class="wide">
  <img src="/figures/phase_diagrams.svg" alt="Transition depth as a function of alpha (placeholder)." />
</figure>

**Figure 1.** Empirical phase diagrams and training accuracy for a deep MLP with 50 layers and hidden dimension $N=50$ (with/without residual connections, LayerNorm, with different activation functions). *Figures reproduced from* [@doshi2023criticalinitializationwidedeep] *with permission from the authors*. **Upper panel**: $\chi_\mathcal{J}^*$ quantifies proximity to criticality: $\chi_\mathcal{J}^* = 1$ corresponds to criticality, $\chi_\mathcal{J}^* > 1$ to the chaotic phase, and $\chi_\mathcal{J}^* < 1$ to the ordered phase. Solid lines indicate the criticality boundary predicted by the infinite-width calculation. **Lower panel**: training accuracy of the deep MLP on FashionMNIST. The dashed white
lines denote the (analytical) critical lines.

[Fig. 1](#fig-phase-diag) (upper panel) shows phase diagrams in the $(\sigma_w, \sigma_b)$ plane for the model in Eq. [(1)](#eq-residual-update), for multiple choices of the nonlinearity $\phi$, multiple values of the residual scaling $\mu$, and with and without LayerNorm. Each point in the phase diagram is characterized by the theoretically computed quantity $\chi_\mathcal{J}^*$, which is *roughly* equal to $e^{1/\xi}$ and will be defined more rigorously below. Criticality is achieved when $\chi_\mathcal{J}^*=1$, while $\chi_\mathcal{J}^* < 1$ ($\chi_\mathcal{J}^* > 1$ ) corresponds to the ordered (chaotic) phase. [Fig. 1](#fig-phase-diag) (lower panel) shows the training accuracy of the same models on FashionMNIST.

We would like to draw the reader’s attention to three points:

1. The setup with pre-LN and $\mu=1$, which is dominant in real models used in practice, is everywhere-critical. No matter how one chooses the initialization variances $(\sigma_w, \sigma_b)$, the backpropagated gradient norms follow a power law.

2. The setup without pre-LN, with $\phi=\text{erf}$ and $\mu=1$ (i.e. Derf with fixed $\alpha=1$), is not everywhere critical, but it is closer to criticality than setups with non-$\tanh$-like activation functions such as ReLU or GELU. In this setup, the APJN grows stretched-exponentially, $\mathcal{J}^{l, l_0}\sim e^{\sqrt{l/\lambda}}$, which is faster than power-law growth but slower than exponential growth.

3. Note how well criticality at initialization correlates with final training accuracy: the more the model deviates from criticality at initialization, the less trainable it is.

<span id="criticality-mean-field"></span>
### 1.2 Criticality: large-width limit and mean-field formalism

Signal-propagation analysis becomes much simpler in the large-width limit. In this regime, each component of the activation vector at layer $l$, $h^l_i$, is a sum of many (approximately) independent terms, so by the central limit theorem it can be treated as Gaussian. Moreover, components with different indices are independent because the rows of the weight matrix $W$ are independent, and they share the same (typically zero) mean and variance due to permutation symmetry across units. Thus, the distribution of the activation vector at layer $l$ is characterized by a single quantity, its variance $\mathcal{K}_l$, so that $h^l \sim \mathcal{N}(0, \mathcal{K}_l I_{N_l})$, where $I_{N_l}$ denotes the $N_l \times N_l$ identity matrix. Knowledge of $\mathcal{K}_l$ then allows one to compute the expectation of any function of $h^l$. The value of $\mathcal{K}_l$ can be computed recursively from Eq. [(1)](#eq-residual-update) as
<span id="eq-variance-recursion" class="eq-anchor"></span>
$$
\begin{equation}
\mathcal{K}_{l+1} = \mu^2 \mathcal{K}_{l} + \sigma^2_w \mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}\left[ \phi(\tilde{h})^2 \right] + \sigma_b^2,
\end{equation}
$$
where $\tilde {h}$ is either $h/\sqrt{\mathcal{K}_l}$ if LayerNorm is used, or $h$ otherwise. Note that in the expectation above, $h$ is a scalar dummy variable representing a single component of the activation vector at layer $l$. Intuitively, $\mathcal{K}_l$ is the squared norm of $h^l$ normalized by width. In the large $N_l$-limit, the fluctuations of this normalized squared norm vanish, so
<span id="eq-variance-definition" class="eq-anchor"></span>
$$
\begin{equation}
\frac{1}{N_l}\sum_{i=1}^{N_l} \left(h_i^l\right)^2 \to \mathcal{K}_l.
\end{equation}
$$
Eq. [(2)](#eq-variance-recursion) can thus be interpreted as a consequence of applying the Pythagorean theorem to the three vectors in Eq. [(1)](#eq-residual-update), which become orthogonal in the large-width limit. The reduction of the dynamics to tracking the single quantity $\mathcal{K}_l$ at each layer is often referred to as mean-field theory at initialization.

Let us now return to the question of gradient propagation. Given two layers $L$ and $l$, with $L>l$, the gradients of a loss function $\mathcal{L}$ at these two layers are related through the Jacobian matrix:
<span id="eq-grad-chain-rule" class="eq-anchor"></span>
$$
\begin{equation}
\frac{\partial \mathcal{L}}{\partial h^{l}} = (J^{L,l})^T \frac{\partial \mathcal{L}}{\partial h^L},
\end{equation}
$$
where the partial Jacobian matrix between layers $l$ and $L$ is defined as
<span id="eq-partial-jacobian" class="eq-anchor"></span>
$$
\begin{equation}
J^{L, l} = \frac{\partial h^L}{\partial h^{l}}.
\end{equation}
$$
Thus, it is natural that the Frobenius norm of this Jacobian, averaged over weight initializations – the APJN, $\mathcal{J}^{L, l} = \mathbb{E}\,\left[\frac{1}{N_L}\, \lVert J^{L, l}\rVert_F^2\right]$ – relates gradient norms at layers $l$ and $L$ [@doshi2023criticalinitializationwidedeep]:
<span id="eq-gradnorm-apjn" class="eq-anchor"></span>
$$
\begin{equation}
\Big\lVert\frac{\partial \mathcal{L}}{\partial h^{l}}\Big\rVert^2_2 = \Big\lVert\frac{\partial \mathcal{L}}{\partial h^L}\Big\rVert^2_2\, \mathcal{J}^{L,l}.
\end{equation}
$$
In the large-width limit, the APJN satisfies the recursion relation [@doshi2023criticalinitializationwidedeep]

<span id="eq-apjn-recursion" class="eq-anchor"></span>
$$
\begin{equation}
\mathcal{J}^{l+1, l_0} = \chi^l_{\mathcal{J}} \mathcal{J}^{l, l_0}.
\end{equation}
$$
Equivalently, $\mathcal{J}^{L, l} = \chi^l_{\mathcal{J}} \mathcal{J}^{L, l+1}$ for $L > l$. The asymptotic behavior of $\chi^l_{\mathcal{J}}$ as $l$ becomes large determines the phase of a deep network: values greater than $1$ correspond to the chaotic phase, in which gradients grow exponentially; values less than $1$ correspond to the ordered phase, in which gradients decay exponentially; and values close to $1$ correspond to criticality. The quantity $\chi^*_{\mathcal{J}}$ used in the figures in the previous section is $\chi^L_{\mathcal{J}}$, where $L$ is the final layer.

For the model in Eq. [(1)](#eq-residual-update) with LayerNorm,
<span id="eq-chi-layernorm" class="eq-anchor"></span>
$$
\begin{equation}
\chi^l_{\mathcal{J}} = \mu^2 + \frac{\sigma^2_w}{\mathcal{K}_l} 
\mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}
\left[ \phi'(h/\sqrt{\mathcal{K}_l})^2 \right]
\end{equation}
$$
and without LayerNorm,
<span id="eq-chi-no-layernorm" class="eq-anchor"></span>
$$
\begin{equation}
\chi^l_{\mathcal{J}} = \mu^2 + \sigma^2_w \mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}
\left[ \phi'(h)^2 \right].
\end{equation}
$$
For derivations of Eqs. [(6)](#eq-gradnorm-apjn)–[(9)](#eq-chi-no-layernorm), we refer the reader to [@doshi2023criticalinitializationwidedeep]. All of these expressions follow directly from the definition of the APJN. Note the additional factor of $\mathcal{K}_l^{-1}$ in Eq. [(8)](#eq-chi-layernorm) relative to Eq. [(9)](#eq-chi-no-layernorm), which arises from the LayerNorm Jacobian. Overall, once $\mathcal{K}_l$ is obtained from the variance-propagation equation [(2)](#eq-variance-recursion), Eq. [(8)](#eq-chi-layernorm) or Eq. [(9)](#eq-chi-no-layernorm) determines the APJN and, consequently, the behavior of the gradient norm.

<span id="analysis-layernorm-free"></span>
## 2. Analysis of criticality in layernorm-free transformers

<span id="layernorm-vs-tanh"></span>
### 2.1 LayerNorm vs. tanh-like nonlinearity: criticality perspective

Let us compare the behavior of $\chi^l_\mathcal{J}$ for the setups with and without LayerNorm. We will stick to the standard practical choices of $\mu = 1$ and $\sigma_b = 0$.

**LayerNorm**:
<span id="eq-layernorm-critical" class="eq-anchor"></span>
$$
\begin{equation}
\chi^l_{\mathcal{J}} = 1 + \frac{\sigma^2_w}{\mathcal{K}_l} 
\mathbb{E}_{\,\tilde{h}\sim \mathcal{N}(0,\, 1)}
\left[ \phi'(\tilde{h})^2 \right],\quad \mathcal{K}_{l+1} = \mathcal{K}_{l} + \sigma^2_w \mathbb{E}_{\,\tilde{h}\sim \mathcal{N}(0,\, 1)}\left[ \phi(\tilde{h})^2 \right].
\end{equation}
$$
Note that we have rewritten the expectations in Eqs. [(2)](#eq-variance-recursion) and [(8)](#eq-chi-layernorm) in terms of a standard Gaussian, making it explicit that they do not depend on $\mathcal{K}_l$. This is consistent with the fact that LayerNorm rescales the activation vector to have norm $\sqrt{N_l}$. At each layer, $\mathcal{K}_l$ receives the same increment and therefore grows linearly with $l$. Consequently, for large $l$, $\chi^l_\mathcal{J}=1+\zeta/l$, where $\zeta > 0$ is a constant determined by Eq. [(10)](#eq-layernorm-critical) and the choice of nonlinearity $\phi$. Thus, the APJN grows as a power law, $\mathcal{J}^{l, l_0} \sim l^{\zeta}$, and so do the squared gradient norms in the backward pass. For $\phi=\text{ReLU}$ or $\phi=\text{id}$, $\zeta =1$.   

**No LayerNorm**:
<span id="eq-no-layernorm-critical" class="eq-anchor"></span>
$$
\begin{equation}
\chi^l_{\mathcal{J}} = 1 + \sigma_w^2
\mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}
\left[ \phi'(h)^2 \right],\quad \mathcal{K}_{l+1} = \mathcal{K}_{l} + \sigma^2_w \mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}\left[ \phi(h)^2 \right].
\end{equation}
$$
In this case, the expectations depend explicitly on $\mathcal{K}_l$, and the behavior of $\chi^l_{\mathcal{J}}$ depends strongly on the choice of nonlinearity $\phi$. 

For $\tanh$-like nonlinearities $\phi$, $\phi(h)^2$ is close to $1$ on the whole real line except in the vicinity of $h=0$. As $\mathcal{K}_l$ grows, the probability mass near $h=0$ decreases and $\mathbb{E}\left[ \phi(h)^2 \right]$ approaches $1$. Thus, $\mathcal{K}_l \sim l$ for large $l$. A more geometric way to see this is to note that once the pre-activations in Eq. (1) are large enough, a $\tanh$-like nonlinearity effectively saturates and acts as a normalization, so the term $W\phi(h)$ has approximately constant norm.

On the other hand, $\phi'(h)^2$ is concentrated near $h=0$. As $\mathcal{K}_l$ grows, the Gaussian density at the origin becomes approximately $1/\sqrt{2\pi \mathcal{K}_l}$, and consequently $\chi^l_{\mathcal{J}}=1 + \sqrt{1/\lambda \mathcal{K}_l}\approx 1 + \sqrt{1/\lambda l}$ for large $l$, where $\lambda$ is a constant determined by Eq. [(11)](#eq-no-layernorm-critical). Thus, the APJN grows stretched-exponentially, $\mathcal{J}^{l, l_0} \sim e^{\sqrt{l/\lambda}}$, and so do the gradient norms in the backward pass.

 We also note that reshaping Eq. [(1)](#eq-residual-update) into a more MLP-like form, as in modern Transformers – e.g., $h^{l+1} = h^l + W_2\phi(W_1\tilde h^{l})$ with a ReLU-like $\phi$ or $h^{l+1} = h^l + W_2\phi_2(W_1\phi_1 (h^{l}))$ with a ReLU-like $\phi_2$ and a $\tanh$-like $\phi_1$ – does not change the results qualitatively. For example, with an intermediate ReLU nonlinearity, this preserves the form of Eq. [(10)](#eq-layernorm-critical) under the replacement $\sigma^2_w\to(\sigma_{w_1}\sigma_{w_2})^2$, and the form of Eq. [(11)](#eq-no-layernorm-critical) under the replacement $\sigma^2_w\to(\sigma_{w_1}\sigma_{w_2})^2/2$.

<span id="introducing-alpha"></span>
### 2.2 Introducing $\alpha$: finite-depth transition from exponential to stretched-exponential behavior 

Let us now consider the Derf nonlinearity as defined in [@chen2025strongernormalizationfreetransformers], i.e. $\phi(x)=\text{erf}(\alpha x)$. The expectations in Eq. [(11)](#eq-no-layernorm-critical) can be computed explicitly:
<span id="eq-derf-recursion" class="eq-anchor"></span>
$$
\begin{equation}
\chi^l_{\mathcal{J}} = 1 + \frac{\sigma_w^2}{\pi}\frac{4\alpha^2}{\sqrt{1+4\alpha^2 K_l}}
,\quad \mathcal{K}_{l+1} = \mathcal{K}_{l} + \sigma^2_w \frac{2}{\pi}\arcsin\left(\frac{2\alpha^2 \mathcal{K}_l}{1+2\alpha^2 \mathcal{K}_l}\right).
\end{equation}
$$
The depth $l_*$, defined by $\alpha^2\mathcal{K}_{l_*} \sim 1$, separates two APJN growth regimes. For $\alpha^2\mathcal{K}_{l}\ll 1$, both $\mathcal{K}_l$ and $\mathcal{J}^{l,l_0}$ grow exponentially at the same rate: $\mathcal{K}_{l+1}\approx \left(1+4\alpha^2\sigma_w^2/\pi\right)\mathcal{K}_l$ and $\chi^l_\mathcal{J}\approx1+4\alpha^2\sigma^2_w/\pi$. For $\alpha^2\mathcal{K}_{l_*}\gg 1$, the growth of $\mathcal{J}^{l,l_0}$ is stretched-exponential, as discussed above. Assuming $\mathcal{K}_0=1$, the transition depth $l_*$ can be roughly estimated from:
<span id="eq-lstar-condition" class="eq-anchor"></span>
$$
\begin{equation}
\alpha^2\mathcal{K}_{l_*}\approx\alpha^2 \left(1+\frac{4\alpha^2\sigma_w^2}{\pi}\right)^{l_*}=1,
\end{equation}
$$
which yields
<span id="eq-lstar-approx" class="eq-anchor"></span>
$$
\begin{equation}
l_* \approx \frac{\log \alpha^{-2}}{\log\left(1+4\alpha^2\sigma_w^2/\pi\right)}.
\end{equation}
$$
The value of $l_*$ decreases as $\alpha$ increases: for large $\alpha$, the transition to stretched-exponential behavior occurs in early layers (possibly in the first layer), whereas for small $\alpha$ the APJN grows exponentially for many layers before entering the stretched-exponential regime.

For a network of finite depth $L$, this implies that if $l_* > L$ (small $\alpha$) one observes only exponential growth; if $l_* < 1$ (large $\alpha$) one observes purely stretched-exponential growth; and if $1<l_*<L$ one observes a transition from exponential to stretched-exponential growth. Note, however, that a larger $\alpha$ always implies stronger signal amplification from the first layer to the last, i.e. a larger $\mathcal{J}^{L, 0}$. [Fig. 2](#fig-theory-grads) shows $\mathcal{K}_l$ and $\mathcal{J}^{L, l}$ computed from the recursion in Eq. [(12)](#eq-derf-recursion) for Derf with multiple values of $\alpha$ ranging from $0.1$ to $1.9$; it also shows $\mathcal{K}_l$ and $\mathcal{J}^{L, l}$ for the pre-LN setup in Eq. [(10)](#eq-layernorm-critical) with $\phi = \text{id}$. 

<figure id="fig-theory-grads" class="wide">
  <img src="/figures/theory_grads.svg" alt="Transition depth as a function of alpha (placeholder)." />
</figure>

**Figure 2.** **(a)** Layer-wise component variance of the activation vector, $\mathcal{K}_l$, and **(b)** the APJN $\mathcal{J}^{L,l}$, which characterizes amplification of the squared gradient norm from layer $L$ to layer $l$ in the **toy model** [(1)](#eq-residual-update) with Derf and pre-LN. For Derf, for each value of $\alpha$, the black dot marks $l_*$, defined by $\alpha^2 \mathcal{K}_{l_*} = 1$, and indicates the transition to the stretched-exponential regime, i.e. the point at which $\mathcal{K}_l$ begins to grow linearly and $\mathcal{J}^{L, l}$ begins to grow stretched-exponentially (in the backward direction). The solid curve corresponds to the stretched-exponential regime ($l > l_*$), while the dashed curve corresponds to the exponential regime ($l < l_*$). The black curve shows the pre-LN setup with $\phi=\text{id}$.

<span id="empirical-dyt"></span>
### 2.3 Empirical gradient norms in a Transformer with DyT

We empirically measure activation and gradient norms at initialization in a ViT model with $128$ layers and hidden dimension $N=1024$, comparing DyT with different values of $\alpha$ to the pre-LN baseline. 
[Fig. 3 (a)](#fig-vit-grads) shows the component-wise activation variance at each layer, $\mathcal{K}_l=\mathbb{E} \, N^{-1}\lVert h^l \rVert^2$. [Fig. 2 (b)](#fig-vit-grads) shows the layer-wise gradient amplification coefficient $\hat{\mathcal{J}}^{L,l}=\mathbb{E}\,\lVert \nabla_{h^l} \mathcal{L}\rVert^2/\mathbb{E}\lVert\nabla_{h^L} \mathcal{L}\rVert^2$. [Fig. 3 (c)](#fig-vit-grads) shows that the squared logarithm of $\hat{\mathcal{J}}^{l,0}=\mathbb{E}\,\lVert \nabla_{h^l} \mathcal{L}\rVert^2/\mathbb{E}\lVert\nabla_{h^0} \mathcal{L}\rVert^2$ is approximately linear in $l$, indicating stretched-exponential growth for larger $\alpha$. Averages are taken over the batch dimension and patches. 

Despite the presence of attention layers in ViT, which makes the analytic treatment more challenging, the qualitative gradient behavior agrees well with the simplified model studied above. In particular, matching the pre-LN gradient amplification behavior requires choosing a smaller $\alpha$. However, smaller values of $\alpha$ in DyT produce smaller updates to the residual stream. Thus, comparing the models purely by gradient amplification can be misleading.
A perhaps more natural comparison is to align the pre-LN setup with DyT by matching the magnitude of the residual stream update. Concretely, one chooses $\alpha$ so that the $\mathcal{K}_l$ curves for pre-LN and DyT are as close as possible, which results in a large $\alpha$. Under this alignment, gradient amplification in DyT is much larger.  

<figure id="fig-vit-grads" class="wide">
  <img src="/figures/vit_grads.svg" alt="Transition depth as a function of alpha (placeholder)." />
</figure>


**Figure 3.** Empirical measurements in a ViT model with $128$ layers and hidden dimension $N=1024$: **(a)** the layer-wise component variance of the activation vector, $\mathcal{K}_l$; and the layer-wise gradient amplification coefficients **(b)** $\hat{\mathcal{J}}^{L,l}=\mathbb{E}\,\lVert \nabla_{h^l} \mathcal{L}\rVert^2/\mathbb{E}\,\lVert\nabla_{h^L} \mathcal{L}\rVert^2$ and **(c)** $\hat{\mathcal{J}}^{l,0}=\mathbb{E}\,\lVert \nabla_{h^l} \mathcal{L}\rVert^2/\mathbb{E}\,\lVert\nabla_{h^0} \mathcal{L}\rVert^2$. Averages are taken over the batch dimension and patches. For Derf, we use the same conventions as in Fig.~1: the dot marks $l_*$ (defined by $\alpha^2 \mathcal{K}_{l_*} = 1$), solid/dashed curves indicate the stretched-exponential/exponential regimes, and the black curve is the pre-LN baseline. Overall, DyT exhibits much faster gradient-norm amplification from later layers to earlier ones; this effect can be mitigated by choosing $\alpha$ sufficiently small for the given model depth.


<span id="dyt-warmup"></span>
### 2.4 DyT benefits from learning rate warm-up

[@zhu2025transformersnormalization; @chen2025strongernormalizationfreetransformers] trained their DyT ViT models with learning-rate warm-up. We empirically show that, in addition to tuning initial $\alpha$, DyT Transformers may require careful tuning of the number of warm-up steps, whereas this is less important for the pre-LN variant. We train a ViT-B ($12$ layers, hidden dimension $N=768$) on CIFAR-100, with DyT and with pre-LN, varying the number of warm-up epochs, the learning rate, and the DyT parameter $\alpha$ at initialization. [Fig. 4 (a)](#fig-warmup) shows that reducing the number of warm-up epochs can destabilize training with DyT, while using too many warm-up epochs can slow training: the model without warm-up diverges, while the model with 3 warm-up epochs converges slightly faster than the model with 10 warm-up epochs. [Fig. 4 (b)](#fig-warmup) compares the DyT variant ($\alpha = 1$) to the pre-LN variant without warm-up as the learning rate is varied. The DyT model with $\alpha=1$ trains stably only at the lowest learning rate, and converges more slowly than the pre-LN baseline. 
As argued in the previous section, it is natural to align pre-LN with DyT based on the size of the residual stream updates; for this reason, we compare pre-LN to the DyT variant with the relatively large value $\alpha=1$ here.
[Fig. 4 (c)](#fig-warmup) shows the test accuracy after 9 epochs as a function of $\alpha$ and the number of warm-up epochs, confirming that that warm-up becomes more important as $\alpha$ increases. Finally, [Fig. 4 (d)](#fig-warmup) shows that choosing $\alpha$ too small can also slow training, possibly due to the smaller residual stream updates.

Overall, a good initialization of $\alpha$ should avoid both extremes: values that are so small that training slows down and values that are so large that training becomes unstable. Increasing the number of warm-up epochs can help stabilize training at larger $\alpha$. The pre-LN setup exhibits better training stability across a wider range of learning rates than DyT, provided that $\alpha$ is chosen so as not to overly reduce the size of the residual updates relative to pre-LN.

<figure id="fig-warmup" class="wide">
  <img src="/figures/vit_warmup.svg" alt="Transition depth as a function of alpha (placeholder)." />
</figure>

**Figure 4.**  Effect of learning-rate warm-up, learning rate, and initialization $\alpha$ on training stability in ViT-B (12 layers, $N=768$) on CIFAR-100 for DyT and pre-LN. **(a)** Warm-up sweep for DyT. **(b)** Learning-rate sweep comparing DyT ($\alpha=1$) to pre-LN without warm-up. **(c)** Test accuracy at epoch 9 versus $\alpha$ and warm-up epochs. **(d)** $\alpha$ sweep. At larger values of $\alpha$, DyT is more sensitive to warm-up and exhibits worse training stability than pre-LN across a range of learning rates.

<span id="conclusion"></span>
## 3. Conclusion



<span id="references"></span>
## 4. References

<div id="refs"></div>


