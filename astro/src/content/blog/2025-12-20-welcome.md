---
title: "Normalization-free transformers are subcritical"
date: 2025-12-20
description: "What do Transformers with pointwise normalization functions trade off to achieve better computational efficiency?"
---

Plan:

0. Brief description of results.
1. Introduction to criticality.
- Intuitive picture of phase diagram
- Criticality directly determines training stability; criticality is a good predictor of final quality.
- Large width limit and simplifications due to it. Intuitive geometric picture – Pythagorean theorem.
- Forward/backward duality.
2. Main 
 - Layernorm vs. DyT/Derf: criticality perspective
 - Boundedness and monotonicity of the point-wise normalization functions from first principles.
 - DyT/Derf – how alpha affects (sub-)criticality.
 - Experiment vs. theory – ViT at initialization.
 - Why can't we just pick a smaller alpha?
3. Conclusion – less compute, less stability.

## Introduction

In this blog post, we connect recent work proposing LayerNorm-free Transformers – in which (pre-)LayerNorm is replaced by pointwise activation functions such as $\tanh$ or $\text{erf}$ – with the body of work studying criticality in wide neural networks at initialization. We note that, in neural networks with residual connections (resnets), $\tanh$-like normalization functions are subcritical, whereas LayerNorm achieves criticality, as previously shown in []. In practical terms, resnets with Dynamic $\tanh$ ($\text{DyT}$) or Dynamic $\text{erf}$ ($\text{Derf}$) normalization exhibit worse gradient propagation than pre-LN resnets: gradients grow exponentially or stretched-exponentially with depth ($e^{l}$ or $e^{\sqrt{l}}$, depending on $\alpha$) rather than following a power law. This can cause training instability and may require more careful hyperparameter tuning to avoid divergence. In addition, existing theory [] shows that subcritical behavior is the best one can achieve with pointwise normalization functions, and that boundedness and monotonicity of the normalization function are required to achieve it. 

We analyze how the parameter $\alpha$, which appears in $\text{DyT}$/$\text{Derf}$, affects gradient propagation in the model proposed in []. For networks of finite depth, smaller values of $\alpha$ give rise to exponentially growing gradients, whereas larger values of $\alpha$ give rise to stretched-exponentially growing gradients. Larger values of $\alpha$ lead to stronger gradient amplification, which explains why they cause training divergence in some experiments. Finally, we show that the qualitative behavior of gradients in ViT aligns well with the theory, even though the theory does not account for attention blocks.

Below, we will

## Criticality


## Analysis of criticality in layernorm-free transformers


##  Conclusion

## References


