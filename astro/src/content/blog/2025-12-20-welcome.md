---
title: "Normalization-free transformers are subcritical"
date: 2025-12-20
description: "What do Transformers with pointwise normalization functions trade off to achieve better computational efficiency?"
---

Plan:

0. Brief description of results.
1. Introduction to criticality.
- Intuitive picture of phase diagram
- Criticality directly determines training stability; criticality is a good predictor of final quality.
- Large width limit and simplifications due to it. Intuitive geometric picture – Pythagorean theorem.
- Forward/backward duality.
2. Main 
 - Layernorm vs. DyT/Derf: criticality perspective
 - Boundedness and monotonicity of the point-wise normalization functions from first principles.
 - DyT/Derf – how alpha affects (sub-)criticality.
 - Experiment vs. theory – ViT at initialization.
 - Why can't we just pick a smaller alpha?
3. Conclusion – less compute, less stability.

I guess my main point is that DyT/Derf require more careful HP tuning – alpha (large lead to divergence, small lead to slow convergence), and lr warm-up. So we will need to show that large alpha -> divergence, small -> slow convergence (the main question – does it hold in the presence of lr warm-up too or only without it?), and the need of lr warm-up

We need to check that pre-ln vit does not require warm up with this lr.

We also explain some empirical observations from the paper from the criticality theory:

1. Smaller alpha -> more stable training (and possible divergence at large alpha)
2. Deeper model -> smaller alpha
3. LLMs are more sensitive to the lr due to the absence of warm-up? Do they train llama with warm-up?

There are some weird statements in their paper like
- depth does not affect optimal alpha -> from their plots, apparently it does.
- non-llms are insensitive to the choice of alpha_0. Ok, seems that they are. Is it due to lr warm-up?

## Abstract

In this blog post, we connect recent work proposing LayerNorm-free Transformers [@zhu2025transformersnormalization] – in which (pre-)LayerNorm is replaced by pointwise activation functions such as $\tanh$ or $\text{erf}$ – with the body of work studying criticality in wide neural networks at initialization. We note that, in neural networks with residual connections (resnets), $\tanh$-like normalization functions are subcritical, whereas LayerNorm achieves criticality, as previously shown in [REF]. In practical terms, resnets with Dynamic Tanh ($\text{DyT}$) or Dynamic erf ($\text{Derf}$) normalization exhibit worse gradient propagation than pre-LN resnets: gradients grow exponentially or stretched-exponentially with depth (from the last layer to the first) rather than following a power law. This can cause training instability and may require more careful hyperparameter tuning to avoid divergence. **In addition, existing theory [REF] shows that subcritical behavior is the best one can achieve with pointwise normalization functions, and that boundedness and monotonicity of the normalization function are required to achieve it.**

We analyze how the initialization of the parameter $\alpha$, which appears in $\text{DyT}$/$\text{Derf}$, affects gradient propagation in the model proposed in [REF]. For networks of finite depth, smaller values of $\alpha$ give rise to exponentially growing gradients, whereas larger values of $\alpha$ give rise to stretched-exponentially growing gradients. We show that the qualitative gradient behavior in ViT aligns well with the theory, even though the theory does not account for attention blocks. Larger values of $\alpha$ lead to stronger gradient amplification, which may explain several empirical observations in $\text{DyT}$/$\text{Derf}$ models: training is typically more stable at smaller $\alpha$, while overly large $\alpha$ can lead to divergence; deeper models require smaller $\alpha$. In addition, we empirically show that $\text{DyT}$/$\text{Derf}$ models benefit from learning rate warm-up, similarly to post-LayerNorm models that suffer from vanishing gradients.

Below, we will

## Background

### Criticality: large picture

It has been known for some time that a network’s signal-propagation properties at initialization directly affect its training stability, which in turn is strongly correlated with the final performance [REF]. As a striking example, modern Transformers commonly use residual connections and pre-LayerNorm (rather than post-LayerNorm or no normalization) – both of which are known to improve gradient propagation and help prevent exponentially vanishing or exploding gradients [REF].

The notion of “good” or “bad” signal propagation can be formalized by introducing the **partial Jacobian** $J^{l, l_0}$ between the model’s representations at layers $l_0$ and $l$, with $l_0<l$. This Jacobian determines how perturbations to activations at layer $l_0$ propagate to layer $l$ in the forward pass and how gradients propagate from layer $l$ back to layer $l_0$ in the backward pass. Its squared Frobenius norm, averaged over weight initializations – $\mathcal{J}^{l, l_0}$ (the **averaged partial Jacobian
norm**, **APJN**) – is the simplest scalar measure that relates the typical gradient magnitudes at layers $l$ and $l_0$ (details below). 

For a given neural network architecture, the partial Jacobian depends on hyperparameters such as the initialization variances of weights and biases, as well as less standard ones such as residual scaling and the parameter $\alpha$ in $\text{DyT}$/$\text{Derf}$. Thus, each point in hyperparameter space may be characterized by the resulting APJN behavior.

If the APJN grows (or decays) exponentially with depth, $\mathcal{J}^{l, l_0}\sim e^{\pm l/\xi}$, the model is said to be in the **chaotic** (or **ordered**) phase. The depth scale $\xi$, called the **correlation length**, determines the effective depth over which signals propagate efficiently. Empirically, it has been shown that when the model depth substantially exceeds $\xi$, training becomes unstable [REF]. Therefore, to achieve stable training, it is desirable to make $\xi$ as large as possible. 

Typically, as one approaches the region of hyperparameters where $\xi$ diverges, the asymptotic APJN scaling switches to a power law, $\mathcal{J}^{l, l_0}\sim l^{\zeta}$, with critical exponent $\zeta$. In this case, the model is said to be at **criticality**.

Let us illustrate these ideas using the figures from [REF]. That work studies a residual network of the form
$$
\begin{equation}
h^{l+1} = \mu h^l + W\phi(\tilde h^{l}) + b,
\end{equation}
$$
where $h^l \in \mathbb{R}^{N_l}$, $W\in \mathbb{R}^{N_{l+1}\times N_l}$, and $b \in \mathbb{R}^{N_{l+1}}$. Here $\tilde {h}^{l}$ is either $\text{LayerNorm}(h^{l})$ or simply $h^{l}$ if no normalization layer is used. We take $h^0$ to be the initial embedding. The components of the weight matrix $W$ are initialized from $\mathcal{N}(0, \sigma_w^2/N_l)$, and the components of the bias vector $b$ are initialized from $\mathcal{N}(0, \sigma_b^2)$. The parameter $\mu$, which is typically set to $1$ in modern transformers, is the residual scaling that controls the contribution of the residual stream. When $\mu = 0$, the model reduces to a feedforward network without residual connections. 

Fig. [REF] (left panel) shows phase diagrams in the $(\sigma_w, \sigma_b)$ plane for the model in Eq. (1), for multiple choices of the nonlinearity $\phi$, multiple values of the residual scaling $\mu$, and with and without LayerNorm. Each point in the phase diagram is characterized by the theoretically computed quantity $\chi_\mathcal{J}^*$, which is *roughly* equal to $e^{1/\xi}$ and will be defined more rigorously below. Criticality is achieved when $\chi_\mathcal{J}^*=1$, while $\chi_\mathcal{J}^* < 1$ ($\chi_\mathcal{J}^* > 1$ ) corresponds to the ordered (chaotic) phase. Fig. [REF] (right panel) shows the training accuracy of the same models on FashionMNIST.

We would like to draw the reader’s attention to three points:

1. The setup with pre-LN and $\mu=1$, which is dominant in real models used in practice, is everywhere-critical. No matter how one chooses the initialization variances $(\sigma_w, \sigma_b)$, the backpropagated gradient norms follow a power law.

2. The setup without pre-LN, with $\phi=\text{erf}$ and $\mu=1$ (i.e. $\text{Derf}$ with fixed $\alpha=1$), is not everywhere critical, but it is closer to criticality than setups with non-$\tanh$-like activation functions such as $\text{ReLU}$ or $\text{GELU}$. In this setup, the APJN grows stretched-exponentially, $\mathcal{J}^{l, l_0}\sim e^{\sqrt{l/\lambda}}$, which is faster than power-law growth but slower than exponential growth.

3. Note how well criticality at initialization correlates with final training accuracy: the more the model deviates from criticality at initialization, the less trainable it is.

### Criticality: large-width limit and mean-field formalism

Signal-propagation analysis becomes much simpler in the large-width limit. In this regime, each component of the activation vector at layer $l$, $h^l_i$, is a sum of many (approximately) independent terms, so by the central limit theorem it can be treated as Gaussian. Moreover, components with different indices are independent because the rows of the weight matrix $W$ are independent, and they share the same (typically zero) mean and variance due to permutation symmetry across units. Thus, the distribution of the activation vector at layer $l$ is characterized by a single quantity, its variance $\mathcal{K}_l$, so that $h^l \sim \mathcal{N}(0, \mathcal{K}_l I_{N_l})$, where $I_{N_l}$ denotes the $N_l \times N_l$ identity matrix. Knowledge of $\mathcal{K}_l$ then allows one to compute the expectation of any function of $h^l$. The value of $\mathcal{K}_l$ can be computed recursively from Eq. (1) as
$$
\begin{equation}
\mathcal{K}_{l+1} = \mu^2 \mathcal{K}_{l} + \sigma^2_w \mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}\left[ \phi(\tilde{h})^2 \right] + \sigma_b^2,
\end{equation}
$$
where $\tilde {h}$ is either $h/\sqrt{\mathcal{K}_l}$ if LayerNorm is used, or $h$ otherwise. Note that in the expectation above, $h$ is a scalar dummy variable representing a single component of the activation vector at layer $l$. Intuitively, $\mathcal{K}_l$ is the squared norm of $h^l$ normalized by width. In the large $N_l$-limit, the fluctuations of this normalized squared norm vanish, so
$$
\begin{equation}
\frac{1}{N_l}\sum_{i=1}^{N_l} \left(h_i^l\right)^2 \to \mathcal{K}_l.
\end{equation}
$$
Eq. (2) can thus be interpreted as a consequence of applying the Pythagorean theorem to the three vectors in Eq. (1), which become orthogonal in the large-width limit. This reduction of the dynamics to tracking the single quantity $\mathcal{K}_l$ is often referred to as mean-field theory at initialization.


Let us now return to the question of gradient propagation. Given two layers $L$ and $l$, with $L>l$, the gradients of a loss function $\mathcal{L}$ at these two layers are related through the Jacobian matrix:
$$
\begin{equation}
\frac{\partial \mathcal{L}}{\partial h^{l}} = (J^{L,l})^T \frac{\partial \mathcal{L}}{\partial h^L},
\end{equation}
$$
where the partial Jacobian matrix between layers $l$ and $L$ is defined as
$$
\begin{equation}
J^{L, l} = \frac{\partial h^L}{\partial h^{l}}.
\end{equation}
$$
Thus, it is natural that the Frobenius norm of this Jacobian, averaged over weight initializations – the APJN, $\mathcal{J}^{L, l} = \mathbb{E}\,\left[\frac{1}{N_L}\, \lVert J^{L, l}\rVert_F^2\right]$ – relates gradient norms at layers $l$ and $L$ [REF]:
$$
\begin{equation}
\Big\lVert\frac{\partial \mathcal{L}}{\partial h^{l}}\Big\rVert^2_2 = \Big\lVert\frac{\partial \mathcal{L}}{\partial h^L}\Big\rVert^2_2\, \mathcal{J}^{L,l}.
\end{equation}
$$
In the large-width limit, the APJN satisfies the recursion relation [REF]
$$
\begin{equation}
\mathcal{J}^{l+1, l_0} = \chi^l_{\mathcal{J}} \mathcal{J}^{l, l_0}.
\end{equation}
$$
Equivalently, $\mathcal{J}^{L, l} = \chi^l_{\mathcal{J}} \mathcal{J}^{L, l+1}$ for $L > l$. The asymptotic behavior of $\chi^l_{\mathcal{J}}$ as $l$ becomes large determines the phase of a deep network: values greater than $1$ correspond to the chaotic phase, in which gradients grow exponentially; values less than $1$ correspond to the ordered phase, in which gradients decay exponentially; and values close to $1$ correspond to criticality. The quantity $\chi^*_{\mathcal{J}}$ used in the figures in the previous section is $\chi^L_{\mathcal{J}}$, where $L$ is the final layer.

For the model in Eq. (1) with LayerNorm,
$$
\begin{equation}
\chi^l_{\mathcal{J}} = \mu^2 + \frac{\sigma^2_w}{\mathcal{K}_l} 
\mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}
\left[ \phi'(h/\sqrt{\mathcal{K}_l})^2 \right]
\end{equation}
$$
and without LayerNorm,
$$
\begin{equation}
\chi^l_{\mathcal{J}} = \mu^2 + \sigma^2_w \mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}
\left[ \phi'(h)^2 \right].
\end{equation}
$$
For derivations of Eqs. (6)–(9), we refer the reader to [REF]. All of these expressions follow directly from the definition of the APJN. Note the additional factor of $\mathcal{K}_l^{-1}$ in Eq. (8) relative to Eq. (9), which arises from the LayerNorm Jacobian. Overall, once $\mathcal{K}_l$ is obtained from the variance-propagation equation (2), Eq. (8) or Eq. (9) determines the behavior of the gradient norm.


## Analysis of criticality in layernorm-free transformers

### LayerNorm vs. tanh-like nonlinearity: criticality perspective

Let us compare the behavior of $\chi^l_\mathcal{J}$ for the setups with and without LayerNorm. We will stick to the standard practical choices of $\mu = 1$ and $\sigma_b = 0$.

**LayerNorm**:
$$
\begin{equation}
\chi^l_{\mathcal{J}} = 1 + \frac{\sigma^2_w}{\mathcal{K}_l} 
\mathbb{E}_{\,\tilde{h}\sim \mathcal{N}(0,\, 1)}
\left[ \phi'(\tilde{h})^2 \right],\quad \mathcal{K}_{l+1} = \mathcal{K}_{l} + \sigma^2_w \mathbb{E}_{\,\tilde{h}\sim \mathcal{N}(0,\, 1)}\left[ \phi(\tilde{h})^2 \right].
\end{equation}
$$
Note that we have rewritten the expectations in Eqs. (2) and (8) in terms of a standard Gaussian, making it explicit that they do not depend on $\mathcal{K}_l$. This is consistent with the fact that LayerNorm rescales the activation vector to have norm $\sqrt{N_l}$. At each layer, $\mathcal{K}_l$ receives the same increment and therefore grows linearly with $l$. Consequently, for large $l$, $\chi^l_\mathcal{J}=1+\zeta/l$, where $\zeta > 0$ is a constant determined by Eq. (10) and the choice of nonlinearity $\phi$. Thus, the APJN grows as a power law, $\mathcal{J}^{l, l_0} \sim l^{\zeta}$, and so do the squared gradient norms in the backward pass. For $\text{ReLU}$, $\zeta =1$.   

**No LayerNorm**:
$$
\begin{equation}
\chi^l_{\mathcal{J}} = 1 + \sigma_w^2
\mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}
\left[ \phi'(h)^2 \right],\quad \mathcal{K}_{l+1} = \mathcal{K}_{l} + \sigma^2_w \mathbb{E}_{\,h\sim \mathcal{N}(0,\, \mathcal{K}_l)}\left[ \phi(h)^2 \right].
\end{equation}
$$
In this case, the expectations depend explicitly on $\mathcal{K}_l$, and the behavior of $\chi^l_{\mathcal{J}}$ depends strongly on the choice of nonlinearity $\phi$. 

For $\tanh$-like nonlinearities $\phi$, $\phi(h)^2$ is close to $1$ on the whole real line except in the vicinity of $h=0$. As $\mathcal{K}_l$ grows, the probability mass near $h=0$ decreases and $\mathbb{E}\left[ \phi(h)^2 \right]$ approaches $1$. Thus, $\mathcal{K}_l \sim l$ for large $l$. A more geometric way to see this is to note that once the pre-activations in Eq. (1) are large enough, a $\tanh$-like nonlinearity effectively saturates and acts as a normalization, so the term $W\phi(h)$ has approximately constant norm.

On the other hand, $\phi'(h)^2$ is concentrated near $h=0$. As $\mathcal{K}_l$ grows, the Gaussian density at the origin becomes approximately $1/\sqrt{2\pi \mathcal{K}_l}$, and consequently $\chi^l_{\mathcal{J}}=1 + \sqrt{1/\lambda \mathcal{K}_l}\approx 1 + \sqrt{1/\lambda l}$ for large $l$, where $\lambda$ is a constant determined by Eq. (11). Thus, the APJN grows stretched-exponentially, $\mathcal{J}^{l, l_0} \sim e^{\sqrt{l/\lambda}}$, and so do the gradient norms in the backward pass.

We also note that reshaping Eq. (1) into a more MLP-like form, as in modern Transformers – e.g., $h^{l+1} = h^l + W_2\phi(W_1\tilde h^{l})$ with a $\text{ReLU}$-like $\phi$ or $h^{l+1} = h^l + W_2\phi_2(W_1\phi_1 (h^{l}))$ with a $\text{ReLU}$-like $\phi_2$ and a $\tanh$-like $\phi_1$ – does not change the results qualitatively. For example, with an intermediate ReLU nonlinearity, this preserves the form of Eqs. (10) and (11) under the replacement $\sigma^2_w\to(\sigma_{w_1}\sigma_{w_2})^2/2$.

### Introducing $\alpha$: finite-depth transition from exponential to stretched-exponential behavior 

Let us now consider the $\text{Derf}$ nonlinearity as defined in [REF], i.e. $\phi(x)=\text{erf}(\alpha x)$. The expectations in Eq. (11) can be computed explicitly:
$$
\begin{equation}
\chi^l_{\mathcal{J}} = 1 + \frac{\sigma_w^2}{\pi}\frac{4\alpha^2}{\sqrt{1+4\alpha^2 K_l}}

,\quad \mathcal{K}_{l+1} = \mathcal{K}_{l} + \sigma^2_w \frac{2}{\pi}\arcsin\left(\frac{2\alpha^2 \mathcal{K}_l}{1+2\alpha^2 \mathcal{K}_l}\right).
\end{equation}
$$
The depth $l_*$, defined by $\alpha^2\mathcal{K}_{l_*} \sim 1$, separates two APJN growth regimes. For $\alpha^2\mathcal{K}_{l_*}\ll 1$, both $\mathcal{K}_l$ and $\mathcal{J}^{l,l_0}$ grow exponentially at the same rate: $\mathcal{K}_{l+1}\approx \left(1+4\alpha^2\sigma_w^2/\pi\right)\mathcal{K}_l$ and $\chi^l_\mathcal{J}\approx1+4\alpha^2\sigma^2_w/\pi$. For $\alpha^2\mathcal{K}_{l_*}\gg 1$, the growth of $\mathcal{J}^{l,l_0}$ is stretched-exponential, as discussed above. The transition depth $l_*$ can be roughly estimated from:
$$
\begin{equation}
\alpha^2\mathcal{K}_{l_*}\approx\alpha^2 \left(1+\frac{4\alpha^2\sigma_w^2}{\pi}\right)^{l_*}=1,
\end{equation}
$$
which yields
$$
\begin{equation}
l_* \approx \frac{\log \alpha^{-2}}{\log\left(1+4\alpha^2\sigma_w^2/\pi\right)}.
\end{equation}
$$
The value of $l_*$ decreases as $\alpha$ increases: for large $\alpha$, the transition to stretched-exponential behavior occurs in early layers (possibly in the first layer), whereas for small $\alpha$ the APJN grows exponentially for many layers before entering the stretched-exponential regime.

For a network of finite depth $L$, this implies that if $l_* > l_*$ (small $\alpha$) one observes only exponential growth; if $l_* < 1$ (large $\alpha$) one observes purely stretched-exponential growth; and if $1<l_*<L$ one observes a transition from exponential to stretched-exponential growth (see [Figure 1](#fig-alpha-transition)). Note, however, that a larger $\alpha$ always implies stronger signal amplification from the first layer to the last, i.e. a larger $\mathcal{J}^{L, 0}$. 

<figure id="fig-alpha-transition">
  <img src="/figures/FIGURE.svg" alt="Transition depth as a function of alpha (placeholder)." />
  <figcaption><strong>Figure 1.</strong> Placeholder caption: transition to stretched-exponential gradient growth happens earlier as \( \alpha \) increases.</figcaption>
</figure>

### Empirical gradient norms in a Transformer with $\text{DyT}$

We empirically measure the gradient norms at initialization in the ViT for different values of $\alpha$. We show that their qualitative behavior agrees well with the simplified model studied above, despite the presence of attention layers that make the analytic analysis harder.

### $\text{DyT}$ benefits from learning rate warm-up ?

Similar to post-LN Transformers, which suffer from vanishing gradients and benefit from learning-rate warm-up, $\text{DyT}$ Transformers benefit from warm-up as well. Fig. [REF] compares the training curves of ViT models that differ in the number of warm-up epochs.    

### Can we just pick very small $\alpha$'s?

As discussed above, larger values of $\alpha$ at initialization lead to stronger gradient amplification, so why not pick a very small $\alpha$? Fig. [REF] shows that overly small $\alpha$ slows convergence. Therefore, a good initialization of $\alpha$ should avoid both extremes: values that are so small that training slows down and values that are so large that training becomes unstable.

### Can we do better with point-wise normalization? (Yes?)

The key point is that the expectations in Eq. (11) cannot decay faster than $1/\sqrt{\mathcal{K}_l}$ for large $\mathcal{K}_l$ (see e.g. [REF]). Intuitively, as $\mathcal{K}_l$ becomes large, an expectation of the form 
$$
\begin{equation}
\frac{1}{\sqrt{2\pi \mathcal{K}_l}} \int f(x)^2 e^{-x^2/2\mathcal{K}_l}dx
\end{equation}
$$
approaches
$$
\begin{equation}
\frac{1}{\sqrt{2\pi \mathcal{K}_l}} \int f(x)^2 dx,
\end{equation}
$$
so if the integral is finite, the asymptotic behavior is $1/\sqrt{\mathcal{K}_l}$. Otherwise, the decay is even slower or there is no decay at all. Thus, the best-case behavior of $\chi^l_\mathcal{J}$ is $\chi^l_{\mathcal{J}}=1 + \sqrt{1/\lambda \mathcal{K}_l}$, which is achieved for nonlinearities $\phi$ such that $\int \phi'(x)^2 dx < \infty$. At the same time, we want to choose $\phi$ to provide the fastest growth of $\mathcal{K}_l$. Note the appearing trade-off: making $\mathcal{K}_l$ grow faster with $l$ typically makes $\chi^l_\mathcal{J}$


##  Conclusion

## References

<div id="refs"></div>


